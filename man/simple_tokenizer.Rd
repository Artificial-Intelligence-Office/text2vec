% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/preprocess.R
\name{simple_tokenizer}
\alias{regexp_tokenizer}
\alias{simple_tokenizer}
\title{simple tokenization function. Perform split by fixed symbol.}
\usage{
simple_tokenizer(txt, split = " ")

regexp_tokenizer(txt, split = "([[:space:]]|[[:punct:]])+", ...)
}
\arguments{
\item{txt}{- \link{character} vector}

\item{split}{- \link{character} split symbol}

\item{...}{- arguments to \link{strsplit} function, which is used internally.}

\item{txt}{- \link{character} vector}

\item{split}{- \link{character} split symbol}
}
\value{
- \link{list} of \link{character} vectors.
Each vector containts tokens.

- \link{list} of \link{character} vectors.
Each element of list containts vector of tokens.
}
\description{
simple tokenization function. Perform split by fixed symbol.

simple tokenization function. Perform split by regular expression.
}
\examples{
doc <- c("first second", "bla bla")
simple_tokenizer(doc)
doc <- c("first  second", "bla, bla, blaa")
regexp_tokenizer(doc, split = '([[:space:]]|[[:punct:]])+')
}

