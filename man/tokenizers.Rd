% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/tokenizers.R
\name{tokenizers}
\alias{regexp_tokenizer}
\alias{tokenizers}
\title{Tokenization functions, which performs string splitting
by regular expression or fixed string.}
\usage{
regexp_tokenizer(string, pattern = boundary("word"))
}
\arguments{
\item{string}{\link{character} vector}

\item{pattern}{\link{character} pattern symbol. Also can be one of \link{modifiers}.}
}
\value{
\link{list} of \link{character} vectors.
Each element of list containts vector of tokens.
}
\description{
Tokenization functions, which performs string splitting
by regular expression or fixed string.
}
\details{
Uses \link{str_split} under the hood. (which build on top of \link{stri_split}).
Actually just a wrapper for \code{str_split} which is very consistent flexible and robust.
See \link{str_split} and \link{modifiers} for details.
}
\examples{
doc <- c("first  second", "bla, bla, blaa")
regexp_tokenizer(doc, pattern = boundary(type = "word"))
#faster, but far less general - perform split by a fixed single whitespace symbol.
simple_tokenizer(doc, pattern = " ")
}

